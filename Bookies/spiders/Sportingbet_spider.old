from scrapy.conf import settings
from scrapy.contrib.spiders import Rule
from scrapy.spider import Spider
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector
from Bookies.items import EventItem
from scrapy.http import Request
import datetime,re
import os
LOG_DIR = settings['LOG_DIR']

today = datetime.datetime.now() #datetime obj will need str later with .strftime('%m %d')


def leagueFilter(name, link):
    #
    # If any of these phrases in the link
    # we deem link as junk, and do not want to scrape it.
    #
    junkPhrases = [
                   'outright',
                   'in-play',
                    'winning-nation',
                    'corners',
                    'reach-the',
                    'to-qualify'

                  ]

    exceptionPhrases = []

    for phrase in exceptionPhrases:
        #don't filter exceptions
        if phrase in link:
            return False

    for phrase in junkPhrases:
        if phrase in link:
            print "[INFO %s]: \033[7m Filtering out link: %s \033[0m" % (name,link[30:])
            return True

    return False  #don't filter rest


class SportingbetSpider(Spider):
    name = "Sportingbet"
    allowed_domains = ["sportingbet.com"]

    # Visit the football homepage first, and get
    # some session cookies
    # def start_requests(self):
    #     yield Request(
    #     url ='http://www.sportingbet.com/services/Multiples.mvc/GetMostPopularSelections' ,
    #     callback=self.i_am_human
    #     )

    # def i_am_human(self,response):
    #     #Looks like Sportingbet have a new 'this-is-human' cookie, set by some js
    #     #The min working cURL is :
    #     #curl 'http://www.sportingbet.com/services/CouponTemplate.mvc/GetCoupon?couponAction
    #     #=EVENTCLASSCOUPON&sportIds=102&marketTypeId=&eventId=&bookId=&eventClassId=737635&sportId=102&eventTimeGroup=ETG_NextFewHours_0_0'
    #     # -H 'Cookie: this-is-human=QNDdscQWhtbhfDIJ4sp17fGLjkaEylIMHMAnsGoBBt9wemrgwfWwEKOySIDbzjMmTh7fOM49AAAAAQ==;'  --compressed
    #     #We can get this cookie simply enough from the redirect html itself!

    #     sel = Selector(response)

    #     human_script = sel.xpath('//body/script/text()').extract()
    #     cookie_line = human_script[0].splitlines()[1].strip()[:-1] #drop final js ';'
    #     human_cookie=cookie_line.split(';')[0]
    #     regex=re.compile(r"this-is-human=(?P<val>.+)")
    #     r = regex.search(human_cookie)
    #     vals = r.groupdict()
    #     try:
    #         cookies={'this-is-human': vals['val']}
    #     except KeyError as e:
    #         print "[ERROR %s]: \033[7m\033[31m KeyError: getting i-am-human cookie \033[0m" % (self.name)
    #         print "[ERROR %s]: \033[7m\033[31m challenge resp dump: %s" % (self.name, response.body)
    #         exit

    #     # With the challenge met, now request leagues
    #     # and proceed as usual
    #     base_url='http://www.sportingbet.com/sports-football/0-102-410.html'
    #     req=Request(url=base_url, cookies=cookies, callback=self.parse_leagues)
    #     req.meta['cookies']=cookies
    #     yield req

    #Looks like the ditched the i-am-human crap?
    def start_requests(self):
        yield Request(url ='http://www.sportingbet.com/sports-football/0-102-410.html' ,
                      callback=self.parse_leagues
                      )

    def parse_leagues(self, response):

        sel = Selector(response)

        sx = SgmlLinkExtractor(allow=[r'http://www.sportingbet.com/sports-football/[A-Za-z0-9-]+/1-102-\d+.html'
                                     ])

        league_links = sx.extract_links(response)

        #remove unwanted links, returns True to filter out link
        league_links = [link for link in league_links if not leagueFilter(self.name, link.url)]


        eventClassIdList=[]
        # Extract eventClassId from the link.url with regex
        for link in league_links:
            with open(os.path.join(LOG_DIR, self.name + '_leagues.log'),'w') as linksf:
                print >> linksf, link.url
            matches=re.findall(r'http://www.sportingbet.com/sports-football/[A-Za-z0-9-]+/1-102-(\d+?).html',link.url)
            if matches:
                eventClassIdList.append(matches[0])


        base_url='http://www.sportingbet.com/services/CouponTemplate.mvc/GetCoupon'
        headers={'Content-Type': 'application/x-www-form-urlencoded',
                     'Referer': 'http://www.sportingbet.com/sports-football/0-102-410.html',
                     'X-Requested-With': 'XMLHttpRequest'
                }
        # cookies =response.meta['cookies']

        for id in eventClassIdList:
            #Build GETstr
            GETstr  = '?couponAction=EVENTCLASSCOUPON&'
            GETstr += 'sportIds=102&'
            GETstr += 'marketTypeId=&'
            GETstr += 'eventId=&'
            GETstr += 'bookId=&'
            GETstr += 'eventClassId='+str(id)+'&'
            GETstr += 'sportId=102&'
            GETstr += 'eventTimeGroup=ETG_NextFewHours_0_0'
            #make req
            req= Request(url=base_url+GETstr, headers=headers,
                         #cookies=cookies,
                         callback=self.parse_Data)
            yield req


    def parse_Data(self, response):

        sel = Selector(response)

        print "[INFO %s]: \033[7m Going to parse data for URL: %s \033[0m" % (self.name,response.url[20:])

        eventSelection = sel.xpath('//div[@class="event active"]')

        items = []
        for event in eventSelection:
               item = EventItem()

               # 22/03/2014 17:30 GMT
               dateandtime = sel.xpath('//div[@class="event active"]/div[@class="columns"]/div[@class="eventInfo"]/span[@class="StartTime"]/text()').extract()

               if dateandtime:
                    # Standarise formatting  to %m %d (09 03).
                    # Convert to datetime obj first then back to desired str.
                    try:
                        #could prob treat this better, but for now just lob
                        #it off, as BST is not recognised.
                        if 'BST' in dateandtime[0]:
                            chop = dateandtime[0].index('BST')
                            dateandtime[0]=dateandtime[0][:chop].strip()
                            date = [datetime.datetime.strptime(dateandtime[0], '%d/%m/%Y %H:%M').strftime('%m %d')]
                            time = [datetime.datetime.strptime(dateandtime[0], '%d/%m/%Y %H:%M').strftime('%H:%M')]
                        else:
                            date = [datetime.datetime.strptime(dateandtime[0], '%d/%m/%Y %H:%M %Z').strftime('%m %d')]
                            time = [datetime.datetime.strptime(dateandtime[0], '%d/%m/%Y %H:%M %Z').strftime('%H:%M')]
                    except ValueError as e:
                        print "[ERROR %s]: \033[7m\033[31m ValueError: %s for URL: %s \033[0m" % (self.name, e, response.url[20:])
                        print "[ERROR %s]: \033[7m\033[31m Tried d b, d.b.Y no luck \033[0m" % (self.name)
                        print "[ERROR %s]: \033[7m\033[31m Date dump: %s" % (self.name, date)
                        continue
               else:
                   # useless with no date, pipe will drop anyway
                   # skip
                   continue

               item['eventDate'] = date
               item['eventTime'] = time

               # Get eventName.
               #item['eventName'] = event.xpath('div[@class="columns"]/div[@class="eventInfo"]/div[@class="eventName"]/text()').extract()
               item['eventName'] = event.xpath('div[@class="columns"]/div[@class="eventInfo"]/div[@class="eventName"]/a/text()').extract()

               # Get prices.
               #odd3 is draw
               item['odd1'] = event.xpath('div[@class="columns"]/div[@class="selections active"]/div[@class="market"]/div[@class="odds home active"]//div[@id="isOffered"]/a/span[@class="priceText wide  EU"]/text()').extract()
               item['odd3'] = event.xpath('div[@class="columns"]/div[@class="selections active"]/div[@class="market"]/div[@class="odds draw active"]//div[@id="isOffered"]/a/span[@class="priceText wide  EU"]/text()').extract()
               item['odd2'] = event.xpath('div[@class="columns"]/div[@class="selections active"]/div[@class="market"]/div[@class="odds away active"]//div[@id="isOffered"]/a/span[@class="priceText wide  EU"]/text()').extract()

               items.append(item)  # validate in pipelines.py

        return items

